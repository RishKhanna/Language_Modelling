{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4c29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d97a5f",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf07b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [open(\"./corpus/Pride and Prejudice - Jane Austen.txt\", \"r\").read(),\n",
    "          open(\"./corpus/Ulysses - James Joyce.txt\",\"r\").read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1e2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_Model:\n",
    "    \n",
    "    def __init__(self, corpus, smoothing):\n",
    "        self.corpus = corpus\n",
    "        self.smoothing = smoothing\n",
    "        self.hist_with_word = {}\n",
    "        self.unigram = {}\n",
    "    \n",
    "    def check_url(self, word):\n",
    "        url_words = [\"https:\", \"http:\", \"www.\", \".co\", \".org\"]\n",
    "        for i in url_words:\n",
    "            if word.find(i)>=0:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def tokens_per_word(self, word):\n",
    "        if word in [\"<HASHTAG>\", \"<MENTION>\", \"<URL>\"]:\n",
    "            return [word]\n",
    "        word_len = len(word)\n",
    "        s, e = word_len-1, 0\n",
    "        for i in range(word_len):\n",
    "            if (word[i].isalnum()) or word[i] in [\"@\", \"#\"]:\n",
    "                s = i\n",
    "                break\n",
    "        for i in range(word_len)[::-1]:\n",
    "            if word[i].isalnum():\n",
    "                e = i\n",
    "                break\n",
    "\n",
    "        if s>=e:\n",
    "            # entire string is non-alphanum\n",
    "            return list(word)\n",
    "        else:\n",
    "            tokens = []\n",
    "            # all elements till first alpha num\n",
    "            for i in list(word)[:s]:\n",
    "                tokens.append(i)\n",
    "            # word\n",
    "            tokens.append(word[s:e+1])\n",
    "            #elements after word\n",
    "            for i in list(word)[e+1:]:\n",
    "                tokens.append(i)\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def Tokenizer(self, in_str):\n",
    "        \"\"\"\n",
    "        Cleaning includes:\n",
    "            -> replace all \\n with space\n",
    "            -> replace multiple spaces with single space\n",
    "            -> seperate punctuation from start/end of words\n",
    "        Handle:\n",
    "            -> Hashtags\n",
    "            -> Mentions\n",
    "            -> URLs\n",
    "        \"\"\"\n",
    "        # Cleaning \n",
    "        corpus = re.sub('\\n',' ',in_str)\n",
    "        corpus = re.sub(' +',' ',corpus)\n",
    "        word_list = corpus.split()\n",
    "        # seperate punctuations\n",
    "        sep_punct = []\n",
    "        for word in word_list:\n",
    "            # generate tokens from everyword\n",
    "            for i in self.tokens_per_word(word):\n",
    "                sep_punct.append(i)\n",
    "\n",
    "        # get tokens and pad with 3 <PAD>\n",
    "        ### Look for mention, hastag, url\n",
    "        tokens = [\"<S>\", \"<S>\", \"<S>\"]\n",
    "        for i in sep_punct:\n",
    "            if i[0]==\"#\":\n",
    "                tokens.append(\"<HASHTAG>\")\n",
    "            elif i[0]==\"@\":\n",
    "                tokens.append(\"<MENTION>\")\n",
    "            elif self.check_url(i):\n",
    "                tokens.append(\"<URL>\")\n",
    "            else:\n",
    "                tokens.append(i)\n",
    "        tokens += [\"</S>\"]\n",
    "        return tokens\n",
    "      \n",
    "    def update_hist_word(self, sentences, n):\n",
    "        if n==1:\n",
    "            for sent in sentences:\n",
    "                for word in sent:\n",
    "                    if word in self.unigram:\n",
    "                        self.unigram[word] +=1\n",
    "                    else:\n",
    "                        self.unigram[word] =1\n",
    "        else:\n",
    "            for sent in sentences:\n",
    "                for i in range(len(sent)+1)[n:]:\n",
    "                    hist, word = \" \".join(sent[i-n:i-1]), sent[i-1]\n",
    "                    # C[hist][word]\n",
    "                    if hist in self.hist_with_word:\n",
    "                        if word in self.hist_with_word[hist]:\n",
    "                            self.hist_with_word[hist][word] += 1\n",
    "                        else:\n",
    "                            self.hist_with_word[hist][word] = 1\n",
    "                    else:\n",
    "                        self.hist_with_word[hist] = {word:1}\n",
    "    \n",
    "    def train(self):\n",
    "        # Get all sentences from corpa\n",
    "        sents = self.corpus.lower()\n",
    "        sents = sents.split(\".\")\n",
    "        sents = [ self.Tokenizer(sent + \".\") for sent in sents]                    \n",
    "        \n",
    "        # 4,3,2,1-gram\n",
    "        self.update_hist_word(sents,4)\n",
    "        self.update_hist_word(sents,3)\n",
    "        self.update_hist_word(sents,2)\n",
    "        self.update_hist_word(sents,1)\n",
    "        \n",
    "    \n",
    "    def P_MLE(self,hist,word):\n",
    "        if len(hist)==0:\n",
    "            if word in self.unigram:\n",
    "                return self.unigram[word]/sum(self.unigram.values())\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            hist = \" \".join(hist)\n",
    "            if hist in self.hist_with_word:\n",
    "                if word in self.hist_with_word[hist]:\n",
    "                    return self.hist_with_word[hist][word]/sum(self.hist_with_word[hist].values())\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "    def one_min_ladmba(self, hist, word):\n",
    "        \"\"\" Part of Witten Bell Smoothing.\"\"\"\n",
    "        hist = \" \".join(hist)\n",
    "        num_types, num_times = 0,0\n",
    "        if hist in self.hist_with_word:\n",
    "            num_types = len(self.hist_with_word[hist])\n",
    "            num_times = sum(self.hist_with_word[hist].values())\n",
    "        if num_types==0:\n",
    "            fin = 1\n",
    "        else:\n",
    "            fin = (num_types)/(num_types+num_times)\n",
    "        return fin\n",
    "\n",
    "    def P_WB(self,hist,word):\n",
    "        lambda_ = 1 - self.one_min_ladmba(hist, word)\n",
    "        if len(hist)==0:\n",
    "            # https://www.ee.columbia.edu/~stanchen/e6884/labs/lab3/x207.html\n",
    "            C_e, N1plus, V = sum(self.unigram.values()), len(self.unigram), len(self.unigram)\n",
    "            fin = (C_e)/(C_e + N1plus)*self.P_MLE([],word) + (N1plus)/(C_e + N1plus)*(1/V)\n",
    "        else:\n",
    "            fin = lambda_*self.P_MLE(hist,word) + (1 - lambda_)*self.P_WB(hist[1:],word)\n",
    "        return fin\n",
    "    \n",
    "    \n",
    "    def T1(self, hist, word,d):\n",
    "        fin = 1\n",
    "        if word in self.hist_with_word[hist]:\n",
    "            fin = max(self.hist_with_word[hist][word]-d, 0)\n",
    "            fin /= sum(self.hist_with_word[hist].values())\n",
    "        else:\n",
    "            fin = 0\n",
    "        return fin\n",
    "    \n",
    "    def lambda_KN(self,hist, d):\n",
    "        fin = d * ( (len(self.hist_with_word[hist])) / (sum(self.hist_with_word[hist].values())) )\n",
    "        return fin    \n",
    "    \n",
    "    def P_KN(self,hist,word):\n",
    "        # https://en.wikipedia.org/wiki/Kneser-Ney_smoothing\n",
    "        d = 0.2  # manually assigned constant\n",
    "        if len(hist)==0:\n",
    "            if word in self.unigram:\n",
    "                fin = (self.unigram[word]-d)/sum(self.unigram.values())\n",
    "                return fin\n",
    "            else:\n",
    "                # https://stats.stackexchange.com/questions/114863/in-kneser-ney-smoothing-how-are-unseen-words-handled\n",
    "                return d/len(self.unigram)  # d/V\n",
    "        else:\n",
    "            hist_str = \" \".join(hist)\n",
    "            if hist_str in self.hist_with_word:\n",
    "                # P_KN(h,w) = T1 + Lambda*P_KN(h[1:],w)\n",
    "                fin = self.T1(hist_str, word, d) + self.lambda_KN(hist_str, d)*self.P_KN(hist[1:], word)\n",
    "                return fin\n",
    "            else:\n",
    "                # Back-Off\n",
    "                return self.P_KN(hist[1:],word)\n",
    "    \n",
    "    \n",
    "    def get_score(self,in_str):\n",
    "        in_str = in_str.lower()\n",
    "        tokens = self.Tokenizer(in_str)\n",
    "        # Find prob of sentence\n",
    "        fin_score = 1\n",
    "        if self.smoothing==\"w\":\n",
    "            for i in range(len(tokens))[3:]:\n",
    "                score = self.P_WB(tokens[i-3:i], tokens[i])\n",
    "                fin_score *= score\n",
    "        elif self.smoothing==\"k\":\n",
    "            for i in range(len(tokens))[3:]:\n",
    "                fin_score *= self.P_KN(tokens[i-3:i], tokens[i])\n",
    "        # Find Perplexity from probablity\n",
    "        # PP = 1/âˆšp\n",
    "        p = fin_score\n",
    "        pp_score = (1/p)**0.5\n",
    "        return pp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e642e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(corpa,r):\n",
    "    \"\"\"\n",
    "    'r' random sentences from the corpus\n",
    "    \"\"\"\n",
    "    sents = corpa.split(\".\")\n",
    "    l = len(sents)\n",
    "    idx_test = random.sample(range(l),r)\n",
    "    X_train, X_test = [], []\n",
    "    for i in range(l):\n",
    "        if i in idx_test:\n",
    "            X_test.append(sents[i])\n",
    "        else:\n",
    "            X_train.append(sents[i])\n",
    "    X_train = \".\".join(X_train)\n",
    "    X_test = \".\".join(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "output_dict, count = {}, 1\n",
    "for corpa in corpus:\n",
    "    train, test = train_test_split(corpa, 1000)\n",
    "    for meathod in [\"k\", \"w\"]:\n",
    "        lm = Language_Model(train, meathod)\n",
    "        lm.train()\n",
    "        \n",
    "        name = \"2019113025_LM\" + str(count)+ \"_train-perplexity.txt\"\n",
    "        output_dict[name] = \"\"\n",
    "        avg = 0\n",
    "        for sent in train.split(\".\"):\n",
    "            try:\n",
    "                score = lm.get_score(sent + \".\")\n",
    "                output_dict[name] += \"\\n\"+str(score)\n",
    "                avg += score\n",
    "            except:\n",
    "                pass\n",
    "        avg /= len(train.split(\".\"))\n",
    "        output_dict[name] = str(avg) + output_dict[name]\n",
    "        \n",
    "        name = \"2019113025_LM\" + str(count)+ \"_test-perplexity.txt\"\n",
    "        output_dict[name] = \"\"\n",
    "        for sent in test.split(\".\"):\n",
    "            try:\n",
    "                score = lm.get_score(sent + \".\")\n",
    "                output_dict[name] += \"\\n\"+str(score)\n",
    "                avg += score\n",
    "            except:\n",
    "                pass\n",
    "        avg /= len(train.split(\".\"))\n",
    "        output_dict[name] = str(avg) + output_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fbaf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_dict:\n",
    "    with open(\"results/\" + i, \"w\") as text_file:\n",
    "        text_file.write(output_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfe5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
