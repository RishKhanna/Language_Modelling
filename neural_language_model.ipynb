{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6753a0-5d58-4e2e-a918-ae9bbf6e5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import numpy as np\n",
    "import random, pickle\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694157d3-c967-4625-a8d4-5125b7a9aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [open(\"./Pride and Prejudice - Jane Austen.txt\", \"r\").read(),\n",
    "          open(\"./Ulysses - James Joyce.txt\",\"r\").read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0194716f-96b4-4f67-853b-6d6e5c9582b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Model:\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    def check_url(self, word):\n",
    "        url_words = [\"https:\", \"http:\", \"www.\", \".co\", \".org\"]\n",
    "        for i in url_words:\n",
    "            if word.find(i)>=0:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def tokens_per_word(self, word):\n",
    "        if word in [\"<HASHTAG>\", \"<MENTION>\", \"<URL>\"]:\n",
    "            return [word]\n",
    "        word_len = len(word)\n",
    "        s, e = word_len-1, 0\n",
    "        for i in range(word_len):\n",
    "            if (word[i].isalnum()) or word[i] in [\"@\", \"#\"]:\n",
    "                s = i\n",
    "                break\n",
    "        for i in range(word_len)[::-1]:\n",
    "            if word[i].isalnum():\n",
    "                e = i\n",
    "                break\n",
    "\n",
    "        if s>=e:\n",
    "            # entire string is non-alphanum\n",
    "            return list(word)\n",
    "        else:\n",
    "            tokens = []\n",
    "            # all elements till first alpha num\n",
    "            for i in list(word)[:s]:\n",
    "                tokens.append(i)\n",
    "            # word\n",
    "            tokens.append(word[s:e+1])\n",
    "            #elements after word\n",
    "            for i in list(word)[e+1:]:\n",
    "                tokens.append(i)\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def Tokenizer(self, in_str):\n",
    "        \"\"\"\n",
    "        Cleaning includes:\n",
    "            -> All sentences have lower case.\n",
    "            -> replace all \\n with space\n",
    "            -> replace multiple spaces with single space\n",
    "            -> seperate punctuation from start/end of words\n",
    "        Handle:\n",
    "            -> Hashtags\n",
    "            -> Mentions\n",
    "            -> URLs\n",
    "        \"\"\"\n",
    "        # Cleaning \n",
    "        in_str = in_str.lower()\n",
    "        corpus = re.sub('\\n',' ',in_str)\n",
    "        corpus = re.sub(' +',' ',corpus)\n",
    "        word_list = corpus.split()\n",
    "        # seperate punctuations\n",
    "        sep_punct = []\n",
    "        for word in word_list:\n",
    "            # generate tokens from everyword\n",
    "            for i in self.tokens_per_word(word):\n",
    "                sep_punct.append(i)\n",
    "\n",
    "        # get tokens and pad with 3 <PAD>\n",
    "        ### Look for mention, hastag, url\n",
    "        tokens = [\"<S>\", \"<S>\", \"<S>\"]\n",
    "        for i in sep_punct:\n",
    "            if i[0]==\"#\":\n",
    "                tokens.append(\"<HASHTAG>\")\n",
    "            elif i[0]==\"@\":\n",
    "                tokens.append(\"<MENTION>\")\n",
    "            elif self.check_url(i):\n",
    "                tokens.append(\"<URL>\")\n",
    "            else:\n",
    "                tokens.append(i)\n",
    "        tokens += [\"</S>\"]\n",
    "        return tokens\n",
    "      \n",
    "    def set_unk(self,tokens_list):\n",
    "        vocab = {}\n",
    "        for tokens in tokens_list:\n",
    "            for token in tokens:\n",
    "                if token in vocab:\n",
    "                    vocab[token] += 1\n",
    "                else:\n",
    "                    vocab[token] = 1\n",
    "        thr = 1\n",
    "        fin = []\n",
    "        for tokens in tokens_list:\n",
    "            fin.append([])\n",
    "            for token in tokens:\n",
    "                if vocab[token]<=thr:\n",
    "                    fin[-1].append(\"<UNK>\")\n",
    "                else:\n",
    "                    fin[-1].append(token)\n",
    "        vocab = {}\n",
    "        for tokens in fin:\n",
    "            for token in tokens:\n",
    "                if token in vocab:\n",
    "                    vocab[token] += 1\n",
    "                else:\n",
    "                    vocab[token] = 1\n",
    "        vocab = list(vocab.keys())\n",
    "        vocab = { vocab[i]:i for i in range(len(vocab)) }\n",
    "        self.vocab = vocab\n",
    "        return fin\n",
    "    \n",
    "    def train_test_split(self,r):\n",
    "        sents = self.corpus.split(\".\")\n",
    "        l = len(sents)\n",
    "        idx_test = random.sample(range(l),int(r*l))\n",
    "        X_train, X_test = [], []\n",
    "        for i in range(l):\n",
    "            if i in idx_test:\n",
    "                X_test.append(sents[i])\n",
    "            else:\n",
    "                X_train.append(sents[i])\n",
    "        X_train = \".\".join(X_train)\n",
    "        X_test = \".\".join(X_test)\n",
    "        self.Xy_train, self.Xy_test = X_train, X_test\n",
    "        return     \n",
    "    \n",
    "    def vocab_vec(self, word, label=\"train\", purpose=\"train\"):\n",
    "        if word in self.vocab:\n",
    "            num = self.vocab[word]\n",
    "        else:\n",
    "            num = self.vocab[\"<UNK>\"]\n",
    "        if label==\"train\":\n",
    "            return num\n",
    "        else:\n",
    "            fin = np.zeros(len(self.vocab))\n",
    "            fin[num] = 1\n",
    "            return fin\n",
    "    \n",
    "    \n",
    "    def gen_ngram(self,tokens_list, score=False):\n",
    "        hist_l, w_l = [], []\n",
    "        for tokens in tokens_list:\n",
    "            for i in range(len(tokens))[3:]:\n",
    "                hist, word = tokens[i-3:i], tokens[i]\n",
    "                t = []\n",
    "                t.append(self.vocab_vec(hist[0]))\n",
    "                t.append(self.vocab_vec(hist[1]))\n",
    "                t.append(self.vocab_vec(hist[2]))\n",
    "                hist_l.append(t)\n",
    "                if score==False:\n",
    "                    w_l.append([self.vocab_vec(word, \"test\")])\n",
    "                else:\n",
    "                    w_l.append([self.vocab_vec(word)])\n",
    "        return hist_l, w_l \n",
    "        \n",
    "    def train(self):\n",
    "        sents = self.Xy_train.split(\".\")\n",
    "        tokens = [self.Tokenizer(sent) for sent in sents]\n",
    "        # To account for unseen words, all words with freq <=thr are <UNK>\n",
    "        tokens = self.set_unk(tokens)\n",
    "        num_classes = len(self.vocab)\n",
    "        # Generate n-grams\n",
    "        X, y = self.gen_ngram(tokens)\n",
    "        X_val, y_val = X[-50:], y[-50:]\n",
    "        X, y = X[:-50], y[:-50]\n",
    "        X, y = np.array(X).astype('int32').reshape((-1,3)), np.array(y).astype('int32').reshape((-1,num_classes))\n",
    "        X_val, y_val = np.array(X_val).astype('int32').reshape((-1,3)), np.array(y_val).astype('int32').reshape((-1,num_classes))\n",
    "        # Neural model stuff\n",
    "        embedding_size = 2048\n",
    "        model1 = Sequential()\n",
    "        model1.add(Embedding(num_classes, embedding_size, input_length=3))\n",
    "        model1.add(LSTM(1024))\n",
    "        model1.add(Dense(800, activation = 'relu'))\n",
    "        model1.add(Dense(num_classes))\n",
    "        model1.compile(loss=BinaryCrossentropy(from_logits=True), optimizer = Adam(1e-4))\n",
    "        model1.fit(X, y, batch_size=400, epochs=3, \n",
    "                       validation_data=(X_val, y_val))\n",
    "        self.model = model1\n",
    "        return\n",
    "    \n",
    "    def get_score(self, in_str):\n",
    "        model = self.model\n",
    "        sents = in_str.split(\".\")\n",
    "        tokens = [self.Tokenizer(sent) for sent in sents]\n",
    "        num_classes = len(self.vocab)\n",
    "        # Generate n-grams\n",
    "        X, y = self.gen_ngram(tokens, score=True)\n",
    "        X = np.array(X).astype('int32').reshape((-1,3))\n",
    "        prob = np.float64(1)\n",
    "        y_pred = model.predict(X, verbose=0)\n",
    "        for i in range(len(y_pred)):\n",
    "            prob *= np.float64(y_pred[i][y[i]]/sum(y_pred[i]))\n",
    "        pp_score = 1/(prob**(1/4))\n",
    "        return pp_score\n",
    "    \n",
    "    def gen_file(self, file_name):\n",
    "        ## gen file for train\n",
    "        print(\"train set\")\n",
    "        fin_str, avg = \"\", 0\n",
    "        sents = self.Xy_train\n",
    "        sents = \" \".join(sents.split(\"\\n\")).split(\".\")\n",
    "        for sent in tqdm(sents):\n",
    "            score = self.get_score(sent)\n",
    "            fin_str += sent + \"\\t\" + str(score) + \"\\n\"\n",
    "            avg += score\n",
    "        avg /= len(sents)\n",
    "        fin_str = str(avg) + \"\\n\" + fin_str\n",
    "        file_name = file_name.split(\"###\")\n",
    "        file_name = \"train\".join(file_name)                                    \n",
    "        with open(file_name, \"w\") as text_file:\n",
    "            text_file.write(fin_str)\n",
    "        ## gen file for test\n",
    "        print(\"test set\")\n",
    "        fin_str, avg = \"\", 0\n",
    "        sents = self.Xy_test.split(\".\")\n",
    "        for sent in tqdm(sents):\n",
    "            score = self.get_score(sent)\n",
    "            fin_str += sent + \"\\t\" + str(score) + \"\\n\"\n",
    "            avg += score\n",
    "        avg /= len(sents)\n",
    "        fin_str = str(avg) + \"\\n\" + fin_str\n",
    "        file_name = file_name.split(\"train\")\n",
    "        file_name = \"test\".join(file_name)                                    \n",
    "        with open(file_name, \"w\") as text_file:\n",
    "            text_file.write(fin_str)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263a3516-9750-4165-b33c-a2cd76f98fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "336/336 [==============================] - 73s 211ms/step - loss: 0.1157 - val_loss: 0.0036\n",
      "Epoch 2/3\n",
      "336/336 [==============================] - 69s 206ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 3/3\n",
      "336/336 [==============================] - 69s 204ms/step - loss: 0.0019 - val_loss: 0.0024\n",
      "train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▎                                     | 339/5924 [00:28<07:18, 12.73it/s]/tmp/ipykernel_22271/2328393792.py:192: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  pp_score = 1/(prob**(1/4))\n",
      "100%|███████████████████████████████████████| 5924/5924 [08:33<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 658/658 [00:56<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......embedding\n",
      ".........vars\n",
      "............0\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-02-18 20:49:22           64\n",
      "variables.h5                                   2023-02-18 20:49:22     99004888\n",
      "config.json                                    2023-02-18 20:49:22         3099\n"
     ]
    }
   ],
   "source": [
    "# Trained on ada\n",
    "nlm = Neural_Model(corpus[0])\n",
    "nlm.train_test_split(0.1)\n",
    "nlm.train()\n",
    "nlm.gen_file(\"./nlp_out/2019113025_LM5_###_perplexity.txt\")\n",
    "with open('./nlp_out/PP_nlm.pkl', 'wb') as file:\n",
    "    pickle.dump(nlm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4e3f29-d6bd-4531-aaa0-3c220f6b9867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 21:03:06.169816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-18 21:03:06.169868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gnode092\n",
      "2023-02-18 21:03:06.169878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gnode092\n",
      "2023-02-18 21:03:06.169945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.108.3\n",
      "2023-02-18 21:03:06.169980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.108.3\n",
      "2023-02-18 21:03:06.169988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.108.3\n",
      "2023-02-18 21:03:06.170269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "356/356 [==============================] - 109s 302ms/step - loss: 0.1098 - val_loss: 0.0022\n",
      "Epoch 2/3\n",
      "356/356 [==============================] - 107s 300ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 3/3\n",
      "356/356 [==============================] - 107s 300ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 3/12015 [00:00<36:22,  5.50it/s]/tmp/ipykernel_40549/2328393792.py:192: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  pp_score = 1/(prob**(1/4))\n",
      "100%|█████████████████████████████████████| 12015/12015 [16:23<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 8010/8010 [11:03<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......embedding\n",
      ".........vars\n",
      "............0\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-02-18 21:36:00           64\n",
      "variables.h5                                   2023-02-18 21:36:00    135608840\n",
      "config.json                                    2023-02-18 21:36:00         3089\n"
     ]
    }
   ],
   "source": [
    "# Trained on ada\n",
    "nlm = Neural_Model(corpus[1])\n",
    "nlm.train_test_split(0.4)\n",
    "nlm.train()\n",
    "nlm.gen_file(\"./nlp_out/2019113025_LM6_###_perplexity.txt\")\n",
    "with open('./nlp_out/U_nlm.pkl', 'wb') as file:\n",
    "    pickle.dump(nlm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d1592c-d927-4b75-a502-9af3a90aa895",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-02-18 20:49:22           64\n",
      "variables.h5                                   2023-02-18 20:49:22     99004888\n",
      "config.json                                    2023-02-18 20:49:22         3099\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......embedding\n",
      ".........vars\n",
      "............0\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109.38301663886185"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./nlp_out/PP_nlm.pkl', 'rb') as file:\n",
    "    loaded_nlm = pickle.load(file)\n",
    "loaded_nlm.get_score(\"was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bbed2a-1383-4585-b44b-7680bf27f989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ----------\n",
      "absl-py                      1.4.0\n",
      "aiofiles                     22.1.0\n",
      "aiosqlite                    0.18.0\n",
      "anyio                        3.6.2\n",
      "argon2-cffi                  21.3.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.2.3\n",
      "asttokens                    2.2.1\n",
      "astunparse                   1.6.3\n",
      "attrs                        22.2.0\n",
      "Babel                        2.11.0\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.11.2\n",
      "bleach                       6.0.0\n",
      "cachetools                   5.3.0\n",
      "certifi                      2022.12.7\n",
      "cffi                         1.15.1\n",
      "charset-normalizer           3.0.1\n",
      "comm                         0.1.2\n",
      "contourpy                    1.0.7\n",
      "cycler                       0.11.0\n",
      "debugpy                      1.6.6\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "executing                    1.2.0\n",
      "fastjsonschema               2.16.2\n",
      "flatbuffers                  23.1.21\n",
      "fonttools                    4.38.0\n",
      "fqdn                         1.5.1\n",
      "gast                         0.4.0\n",
      "google-auth                  2.16.0\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.51.1\n",
      "h5py                         3.8.0\n",
      "idna                         3.4\n",
      "importlib-metadata           6.0.0\n",
      "importlib-resources          5.10.2\n",
      "ipykernel                    6.21.2\n",
      "ipython                      8.10.0\n",
      "ipython-genutils             0.2.0\n",
      "ipywidgets                   8.0.4\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.18.2\n",
      "Jinja2                       3.1.2\n",
      "json5                        0.9.11\n",
      "jsonpointer                  2.3\n",
      "jsonschema                   4.17.3\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.0.2\n",
      "jupyter-console              6.5.1\n",
      "jupyter_core                 5.2.0\n",
      "jupyter-events               0.5.0\n",
      "jupyter_server               2.3.0\n",
      "jupyter_server_fileid        0.6.0\n",
      "jupyter_server_terminals     0.4.4\n",
      "jupyter_server_ydoc          0.6.1\n",
      "jupyter-ydoc                 0.2.2\n",
      "jupyterlab                   3.6.1\n",
      "jupyterlab-pygments          0.2.2\n",
      "jupyterlab_server            2.19.0\n",
      "jupyterlab-widgets           3.0.5\n",
      "keras                        2.11.0\n",
      "kiwisolver                   1.4.4\n",
      "lab                          7.2\n",
      "libclang                     15.0.6.1\n",
      "Markdown                     3.4.1\n",
      "MarkupSafe                   2.1.2\n",
      "matplotlib                   3.7.0\n",
      "matplotlib-inline            0.1.6\n",
      "mistune                      2.0.5\n",
      "nbclassic                    0.5.1\n",
      "nbclient                     0.7.2\n",
      "nbconvert                    7.2.9\n",
      "nbformat                     5.7.3\n",
      "nest-asyncio                 1.5.6\n",
      "notebook                     6.5.2\n",
      "notebook_shim                0.2.2\n",
      "numpy                        1.24.2\n",
      "oauthlib                     3.2.2\n",
      "opt-einsum                   3.3.0\n",
      "packaging                    23.0\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "pexpect                      4.8.0\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       9.4.0\n",
      "pip                          23.0\n",
      "platformdirs                 3.0.0\n",
      "prometheus-client            0.16.0\n",
      "prompt-toolkit               3.0.36\n",
      "protobuf                     3.19.6\n",
      "psutil                       5.9.4\n",
      "ptyprocess                   0.7.0\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.4.8\n",
      "pyasn1-modules               0.2.8\n",
      "pycparser                    2.21\n",
      "Pygments                     2.14.0\n",
      "pyparsing                    3.0.9\n",
      "pyrsistent                   0.19.3\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.6\n",
      "pytz                         2022.7.1\n",
      "PyYAML                       6.0\n",
      "pyzmq                        25.0.0\n",
      "qtconsole                    5.4.0\n",
      "QtPy                         2.3.0\n",
      "regex                        2022.10.31\n",
      "requests                     2.28.2\n",
      "requests-oauthlib            1.3.1\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rsa                          4.9\n",
      "Send2Trash                   1.8.0\n",
      "setuptools                   65.6.3\n",
      "simplejson                   3.18.3\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soupsieve                    2.4\n",
      "stack-data                   0.6.2\n",
      "tensorboard                  2.11.2\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.11.0\n",
      "tensorflow-estimator         2.11.0\n",
      "tensorflow-io-gcs-filesystem 0.30.0\n",
      "termcolor                    2.2.0\n",
      "terminado                    0.17.1\n",
      "tinycss2                     1.2.1\n",
      "tomli                        2.0.1\n",
      "tornado                      6.2\n",
      "tqdm                         4.64.1\n",
      "traitlets                    5.9.0\n",
      "txt2tags                     3.7\n",
      "typing_extensions            4.5.0\n",
      "uri-template                 1.2.0\n",
      "urllib3                      1.26.14\n",
      "wcwidth                      0.2.6\n",
      "webcolors                    1.12\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.5.1\n",
      "Werkzeug                     2.2.3\n",
      "wheel                        0.37.1\n",
      "widgetsnbextension           4.0.5\n",
      "wrapt                        1.14.1\n",
      "y-py                         0.5.5\n",
      "ypy-websocket                0.8.2\n",
      "zipp                         3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94b3bc-d087-469d-a89e-f318752fa82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
